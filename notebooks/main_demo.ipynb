{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "f27944ae",
            "metadata": {},
            "source": [
                "# PrÃ¡ctica Final: Asistente TurÃ­stico de Tenerife (RAG)\n",
                "\n",
                "Notebook de desarrollo paso a paso.\n",
                "\n",
                "## 1. ConfiguraciÃ³n Inicial\n",
                "VerificaciÃ³n de que el entorno y las claves API estÃ¡n correctamente cargadas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8951c85e",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# 1. Configuramos el path para poder importar mÃ³dulos de src/\n",
                "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
                "if project_root not in sys.path:\n",
                "    sys.path.append(project_root)\n",
                "\n",
                "# 2. Cargamos variables de entorno\n",
                "load_dotenv(os.path.join(project_root, '.env'))\n",
                "\n",
                "# 3. VerificaciÃ³n\n",
                "print(f\"Directorio RaÃ­z: {project_root}\")\n",
                "\n",
                "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
                "if api_key and api_key.startswith(\"sk-\"):\n",
                "    print(\"âœ… API Key de OpenAI cargada correctamente.\")\n",
                "else:\n",
                "    print(\"âŒ ERROR: No se detectÃ³ la API Key o el formato es incorrecto.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "test_api_connection",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_openai import ChatOpenAI\n",
                "\n",
                "print(\"â³ Probando conexiÃ³n con OpenAI...\")\n",
                "try:\n",
                "    # Instanciamos el modelo bÃ¡sico\n",
                "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
                "    response = llm.invoke(\"Di 'ConexiÃ³n exitosa' si me lees.\")\n",
                "    print(f\"âœ… Respuesta del LLM: {response.content}\")\n",
                "except Exception as e:\n",
                "    print(f\"âŒ Error de conexiÃ³n con OpenAI: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ingesta_header",
            "metadata": {},
            "source": [
                "## 2. Ingesta de Datos (Lectura PDF)\n",
                "Cargaremos la guÃ­a turÃ­stica desde la carpeta `data/raw` y procesaremos su contenido."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_pdf",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "\n",
                "# Ruta al archivo PDF (Ahora en data/raw)\n",
                "pdf_path = os.path.join(project_root, \"data\", \"raw\", \"TENERIFE.pdf\")\n",
                "\n",
                "print(f\"ðŸ“„ Buscando PDF en: {pdf_path}\")\n",
                "\n",
                "if os.path.exists(pdf_path):\n",
                "    loader = PyPDFLoader(pdf_path)\n",
                "    docs = loader.load()\n",
                "    print(f\"âœ… PDF Cargado con Ã©xito. Total pÃ¡ginas: {len(docs)}\")\n",
                "    \n",
                "    # Verificamos contenido de una pÃ¡gina al azar (pÃ¡g 10, Ã­ndice 9)\n",
                "    print(\"\\n--- Muestra de contenido (PÃ¡g 10) ---\")\n",
                "    print(docs[9].page_content[:500] + \"...\")\n",
                "else:\n",
                "    print(\"âŒ ERROR: El archivo PDF no existe en la ruta especificada.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "split_text",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "\n",
                "# Dividimos el texto en fragmentos (chunks) para que entren en el contexto del LLM\n",
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=1000,    # Caracteres por chunk\n",
                "    chunk_overlap=200,  # Solapamiento para no perder contexto entre cortes\n",
                "    length_function=len,\n",
                "    add_start_index=True,\n",
                ")\n",
                "\n",
                "chunks = text_splitter.split_documents(docs)\n",
                "print(f\"âœ… Documento dividido en {len(chunks)} fragmentos (chunks).\")\n",
                "print(f\"TamaÃ±o promedio del chunk: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f} caracteres.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "refactor_check",
            "metadata": {},
            "source": [
                "## 3. ValidaciÃ³n del MÃ³dulo `src.data.loader`\n",
                "Ahora verificamos que el cÃ³digo refactorizado en `src/` funciona igual que el cÃ³digo experimental de arriba."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "use_loader_class",
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.data.loader import DataLoader\n",
                "\n",
                "print(\"ðŸ”„ Probando clase DataLoader refactorizada...\")\n",
                "# Inicializamos el loader con la misma ruta\n",
                "loader = DataLoader(pdf_path)\n",
                "\n",
                "# Documentos\n",
                "docs_refactored = loader.load()\n",
                "\n",
                "# Chunks\n",
                "chunks_refactored = loader.split(docs_refactored)\n",
                "\n",
                "print(f\"\\nâœ… ValidaciÃ³n completada: {len(chunks_refactored)} chunks generados mediante la clase encapsulada.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vector_store_header",
            "metadata": {},
            "source": [
                "## 4. Motor Vectorial (Embeddings)\n",
                "Convertiremos los chunks de texto en vectores numÃ©ricos y los almacenaremos en ChromaDB."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "create_vector_store",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.vectorstores import Chroma\n",
                "from langchain_openai import OpenAIEmbeddings\n",
                "import shutil\n",
                "\n",
                "# Ruta donde guardaremos la base de datos vectorial\n",
                "chroma_path = os.path.join(project_root, \"chroma_db\")\n",
                "\n",
                "# Limpiamos la BD anterior si existe para empezar de cero\n",
                "if os.path.exists(chroma_path):\n",
                "    print(\"ðŸ§¹ Limpiando base de datos vectorial existente...\")\n",
                "    shutil.rmtree(chroma_path)\n",
                "\n",
                "print(\"ðŸ§  Generando embeddings... (Esto puede tardar un poco)\")\n",
                "\n",
                "# Inicializamos Embeddings y VectorStore\n",
                "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
                "vector_store = Chroma.from_documents(\n",
                "    documents=chunks_refactored,\n",
                "    embedding=embeddings,\n",
                "    persist_directory=chroma_path\n",
                ")\n",
                "\n",
                "print(f\"âœ… Base de datos vectorial creada en: {chroma_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "test_search",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prueba de bÃºsqueda semÃ¡ntica\n",
                "query = \"Â¿QuÃ© altura tiene el Teide?\"\n",
                "print(f\"ðŸ” Buscando: '{query}'\")\n",
                "\n",
                "results = vector_store.similarity_search(query, k=3)\n",
                "\n",
                "for i, res in enumerate(results):\n",
                "    print(f\"\\n--- Resultado {i+1} ---\")\n",
                "    print(res.page_content[:300] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vector_store_refactor_check",
            "metadata": {},
            "source": [
                "## 5. ValidaciÃ³n del MÃ³dulo `src.data.vector_store`\n",
                "Verificamos que la clase `VectorStoreManager` gestiona correctamente la creaciÃ³n y bÃºsqueda."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "use_vector_manager",
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.data.vector_store import VectorStoreManager\n",
                "\n",
                "print(\"ðŸ”„ Probando VectorStoreManager refactorizada...\")\n",
                "\n",
                "chroma_path_ref = os.path.join(project_root, \"chroma_db_refactored\")\n",
                "\n",
                "manager = VectorStoreManager(persist_directory=chroma_path_ref)\n",
                "\n",
                "# Crear DB\n",
                "manager.create_vector_store(chunks_refactored, reset=True)\n",
                "\n",
                "# Buscar\n",
                "results_ref = manager.search(\"Â¿DÃ³nde se encuentra el auditorio AdÃ¡n MartÃ­n?\")\n",
                "\n",
                "for i, res in enumerate(results_ref):\n",
                "    print(f\"\\n--- [Refactor] Resultado {i+1} ---\")\n",
                "    print(res.page_content[:300] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "rag_logic_header",
            "metadata": {},
            "source": [
                "## 6. LÃ³gica RAG (Retrieval Augmented Generation)\n",
                "Implementamos la cadena completa: Pregunta -> BÃºsqueda de Contexto -> Prompt con Contexto -> Respuesta LLM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rag_setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.runnables import RunnablePassthrough\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "\n",
                "# 1. Definimos el Prompt del Asistente\n",
                "template = \"\"\"\n",
                "Eres un guÃ­a turÃ­stico experto en Tenerife, amigable y conocedor.\n",
                "Usa la siguiente informaciÃ³n de contexto para responder a la pregunta del turista.\n",
                "Si no encuentras la respuesta en el contexto, di amablemente que no tienes esa informaciÃ³n, no inventes nada.\n",
                "\n",
                "Contexto:\n",
                "{context}\n",
                "\n",
                "Pregunta del Turista: {question}\n",
                "\n",
                "Respuesta:\n",
                "\"\"\"\n",
                "prompt = ChatPromptTemplate.from_template(template)\n",
                "\n",
                "# 2. Obtenemos el Retriever (Buscador) de nuestra clase VectorStoreManager\n",
                "# Usaremos 'manager' que validamos en el paso anterior y que ya tiene los datos cargados\n",
                "retriever = manager.get_retriever(k=4)\n",
                "\n",
                "# 3. Definimos la Cadena RAG (Chain)\n",
                "def format_docs(docs):\n",
                "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
                "\n",
                "rag_chain = (\n",
                "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
                "    | prompt\n",
                "    | llm  # Usamos el 'llm' que instanciamos al pricipio (ChatOpenAI)\n",
                "    | StrOutputParser()\n",
                ")\n",
                "\n",
                "print(\"âœ… Cadena RAG construida exitosamente.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rag_test",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prueba Real\n",
                "pregunta = \"Â¿QuÃ© planes culturales recomiendas en Santa Cruz?\"\n",
                "\n",
                "print(f\"ðŸ‘¤ Pregunta: {pregunta}\\n\")\n",
                "print(\"ðŸ¤– Generando respuesta...\\n\")\n",
                "\n",
                "respuesta = rag_chain.invoke(pregunta)\n",
                "\n",
                "print(\"ðŸ’¬ Respuesta del Asistente:\")\n",
                "print(respuesta)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "rag_refactor_check",
            "metadata": {},
            "source": [
                "## 7. ValidaciÃ³n del MÃ³dulo `src.core.rag_engine`\n",
                "Ahora probamos la implementaciÃ³n modular de la clase `RagEngine`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "test_rag_engine",
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.core.rag_engine import RagEngine\n",
                "\n",
                "print(\"ðŸ”„ Probando RagEngine refactorizada...\")\n",
                "\n",
                "# Instanciamos el motor usando el manager ya existente\n",
                "rag_engine = RagEngine(vector_store_manager=manager)\n",
                "\n",
                "pregunta_2 = \"Â¿DÃ³nde puedo ver ballenas en Tenerife?\"\n",
                "print(f\"\\nðŸ‘¤ Pregunta: {pregunta_2}\")\n",
                "\n",
                "respuesta_2 = rag_engine.get_response(pregunta_2)\n",
                "\n",
                "print(\"\\nðŸ’¬ Respuesta del Asistente [desde mÃ³dulo]:\")\n",
                "print(respuesta_2)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "memory_header",
            "metadata": {},
            "source": [
                "## 8. Agente Conversacional con Memoria\n",
                "Ahora vamos a permitir que el asistente recuerde la conversaciÃ³n. Usaremos `RunnableWithMessageHistory` de LangChain."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "memory_setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.chat_message_histories import ChatMessageHistory\n",
                "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
                "from langchain_core.prompts import MessagesPlaceholder\n",
                "from operator import itemgetter\n",
                "\n",
                "# 1. Actualizamos el Prompt para incluir historial\n",
                "memory_template = \"\"\"\n",
                "Eres un guÃ­a turÃ­stico experto en Tenerife, amigable y conocedor.\n",
                "Usa la siguiente informaciÃ³n de contexto para responder a la pregunta del turista.\n",
                "Si no encuentras la respuesta en el contexto, di amablemente que no tienes esa informaciÃ³n, no inventes nada.\n",
                "\n",
                "Contexto:\n",
                "{context}\n",
                "\n",
                "Historial de Chat:\n",
                "{chat_history}\n",
                "\n",
                "Pregunta del Turista: {question}\n",
                "\n",
                "Respuesta:\n",
                "\"\"\"\n",
                "memory_prompt = ChatPromptTemplate.from_template(\n",
                "    memory_template\n",
                ")\n",
                "\n",
                "# 2. Creamos nueva chain con 'chat_history'. \n",
                "# USAMOS itemgetter(\"question\") PARA QUE EL RETRIEVER SOLO RECIBA LA PREGUNTA\n",
                "rag_chain_with_history_base = (\n",
                "    {\n",
                "        \"context\": itemgetter(\"question\") | retriever | format_docs, \n",
                "        \"question\": itemgetter(\"question\"), \n",
                "        \"chat_history\": itemgetter(\"chat_history\")\n",
                "    }\n",
                "    | memory_prompt\n",
                "    | llm\n",
                "    | StrOutputParser()\n",
                ")\n",
                "\n",
                "# *Nota: Gestionaremos el historial manualmente para esta demo simple*\n",
                "print(\"âœ… Prompt actualizado para soportar historial.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "memory_test_manual",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulamos una conversaciÃ³n manual\n",
                "chat_history_str = \"\"\n",
                "\n",
                "# Turno 1\n",
                "q1 = \"Â¿QuÃ© tiempo hace en el sur de la isla?\"\n",
                "print(f\"ðŸ‘¤ Pregunta 1: {q1}\")\n",
                "# Invocamos pasando el historial vacÃ­o\n",
                "r1 = rag_chain_with_history_base.invoke({\"question\": q1, \"chat_history\": chat_history_str})\n",
                "print(f\"ðŸ¤– Respuesta 1: {r1}\\n\")\n",
                "\n",
                "# Actualizamos historial (Simulado)\n",
                "chat_history_str += f\"Humano: {q1}\\nAsistente: {r1}\\n\"\n",
                "\n",
                "# Turno 2 (Pregunta contextual)\n",
                "q2 = \"Â¿Y quÃ© playas recomiendas allÃ­? ('allÃ­' se refiere al sur)\"\n",
                "print(f\"ðŸ‘¤ Pregunta 2 (Contextual): {q2}\")\n",
                "r2 = rag_chain_with_history_base.invoke({\"question\": q2, \"chat_history\": chat_history_str})\n",
                "print(f\"ðŸ¤– Respuesta 2: {r2}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "agent_refactor_check",
            "metadata": {},
            "source": [
                "## 9. ValidaciÃ³n del MÃ³dulo `src.core.agent`\n",
                "Ahora probamos la clase `TouristAgent` que encapsula automÃ¡ticamente todo el manejo de memoria."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "use_agent_class",
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.core.agent import TouristAgent\n",
                "\n",
                "print(\"ðŸ”„ Probando TouristAgent (Refactorizado)...\")\n",
                "\n",
                "# Instanciamos el Agente\n",
                "agent = TouristAgent(vector_store_manager=manager)\n",
                "\n",
                "# SesiÃ³n A: Preguntas sobre el Norte\n",
                "session_id = \"sesion_usuario_1\"\n",
                "resp_1 = agent.get_response(\"Â¿QuÃ© se puede visitar en Puerto de la Cruz?\", session_id=session_id)\n",
                "print(f\"\\nðŸ¤– R1: {resp_1}\\n\")\n",
                "\n",
                "resp_2 = agent.get_response(\"Â¿Y quÃ© tal el clima allÃ­?\", session_id=session_id)\n",
                "print(f\"ðŸ¤– R2 (Contextual): {resp_2}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
